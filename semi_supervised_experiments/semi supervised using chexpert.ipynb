{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from core import * \n",
    "from data_manipulation import Transform, RandomRotation, Flip, RandomCrop, normalize_imagenet, normalize_mura, center_crop\n",
    "from utils import save_model, load_model, lr_loss_plot\n",
    "from architectures import DenseNet121\n",
    "from train_functions import OptimizerWrapper, TrainingPolicy, FinderPolicy, validate_multilabel, lr_finder, validate_binary, TTA_binary\n",
    "import json\n",
    "\n",
    "SEED = 42\n",
    "R_PIX = 8\n",
    "IDX = 10 # Emphysema\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "TRANSFORMATIONS = [RandomRotation(arc_width=20), Flip(), RandomCrop(r_pix=R_PIX)]\n",
    "NORMALIZE = True # ImageNet\n",
    "FREEZE = True\n",
    "GRADUAL_UNFREEZING = True\n",
    "n_samples = [50,100,200,400,600,800,1000]\n",
    "\n",
    "\n",
    "\n",
    "BASE_PATH = Path('../..')\n",
    "PATH = BASE_PATH/'data'\n",
    "CHESTXRAY_FOLDER = PATH/'ChestXRay-250'\n",
    "CHEXPERT_FOLDER = PATH/'ChesXPert-250'\n",
    "\n",
    "SAVE_DIRECTORY = Path('./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised\n",
    "chesxray_train_df = pd.read_csv(PATH/\"train_df.csv\")\n",
    "chesxray_valid_df = pd.read_csv(PATH/\"val_df.csv\")\n",
    "chesxray_test_df = pd.read_csv(PATH/\"test_df.csv\")\n",
    "\n",
    "# Unsupervised\n",
    "chexpert_train_df = pd.read_csv(PATH/\"CheXpert-v1.0-small/train.csv\")\n",
    "chexpert_valid_df = pd.read_csv(PATH/\"CheXpert-v1.0-small/valid.csv\")\n",
    "chexpert_train_df = chexpert_train_df[chexpert_train_df['Frontal/Lateral']==\"Frontal\"]\n",
    "chexpert_valid_df = chexpert_valid_df[chexpert_valid_df['Frontal/Lateral']==\"Frontal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data frame labeled data subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(df_col):\n",
    "    return np.array(list(map(np.array, df_col.str.split(' ')))).astype(int)\n",
    "\n",
    "def subset_df(df, amt=None, idx=IDX):\n",
    "    \n",
    "    lbls = decode_labels(df.Label)\n",
    "    \n",
    "    if amt is None: amt=2*lbls[:,idx].sum()\n",
    "        \n",
    "#     df.Label = lbls[:,idx].astype(int)\n",
    "    pos_idxs = lbls[:,idx].astype(bool)\n",
    "\n",
    "    neg = df[~pos_idxs].sample(n=amt//2, replace=False)\n",
    "    pos = df[pos_idxs].sample(n=amt//2, replace=False)\n",
    "\n",
    "    return pd.concat([neg, pos]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Basic Images DataSet\n",
    "\n",
    "    Args:\n",
    "        dataframe with data: image_file, label\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, image_path, idx):\n",
    "        self.image_files = df[\"ImageIndex\"].values\n",
    "        self.lables = np.array([obs.split(\" \")[idx]\n",
    "                                for obs in df.Label]).astype(np.float32)\n",
    "        self.image_path = image_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_path / self.image_files[index]\n",
    "        x = cv2.imread(str(path)).astype(np.float32)\n",
    "        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB) / 255\n",
    "        y = self.lables[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    \n",
    "class UnlabeledDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Basic Images DataSet\n",
    "\n",
    "    Args:\n",
    "        dataframe with data: image_file, label\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, image_path, N):\n",
    "        self.image_files = ['_'.join(p.split('/')[1:]) for p in df[\"Path\"].values]\n",
    "        self.image_path = image_path\n",
    "        self.N = N\n",
    "        self._replace = True if N > len(self.image_files) else False\n",
    "        \n",
    "        self.randomize()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_path / self.iter_image_files[index]\n",
    "        x = cv2.imread(str(path)).astype(np.float32)\n",
    "        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB) / 255\n",
    "\n",
    "        return x, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def randomize(self): self.iter_image_files = np.random.choice(self.image_files, size=self.N, replace=self._replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledTransform():\n",
    "    \"\"\" Rotates an image by deg degrees\n",
    "\n",
    "    Args:\n",
    "\n",
    "        dataset: A base torch.utils.data.Dataset of images\n",
    "        transforms: list with all the transformations involving randomnes\n",
    "\n",
    "        Ex:\n",
    "            ds_transform = Transform(ds, [random_crop(240, 240), rotate_cv()])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transforms=None, normalize=True, seed=42, r_pix=8):\n",
    "        self.dataset, self.transforms = dataset, transforms\n",
    "\n",
    "        if normalize is True: self.normalize = normalize_imagenet\n",
    "        elif normalize=='MURA': self.normalize = normalize_mura\n",
    "        else: self.normalize = False\n",
    "\n",
    "        self.center_crop = partial(center_crop, r_pix=r_pix)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Do transformation when image is called.\n",
    "        We are assuming the trainingvalidation set is read from a folder of images already\n",
    "        noramlized and resized to before random-crop and after random-crop sizes respectively.\n",
    "\n",
    "        \"\"\"\n",
    "        data, label = self.dataset[index]\n",
    "        \n",
    "        out = np.copy(data)\n",
    "\n",
    "        if self.transforms:\n",
    "            for choices, f in list(zip(self.choices, self.transforms)):\n",
    "                args = {k: v[index] for k, v in choices.items()}\n",
    "                out = f(out, **args)\n",
    "        else:\n",
    "            out=self.center_crop(im=out)\n",
    "        \n",
    "        data = self.center_crop(data)\n",
    "\n",
    "        if self.normalize: \n",
    "            out = self.normalize(out)\n",
    "            data = self.normalize(data)\n",
    "            \n",
    "        return np.rollaxis(out, 2), np.rollaxis(data, 2)\n",
    "    \n",
    "    def randomize(self): self.dataset.randomize()\n",
    "\n",
    "    def set_random_choices(self):\n",
    "        \"\"\"\n",
    "        To be called at the begining of every epoch to generate the random numbers\n",
    "        for all iterations and transformations.\n",
    "        \"\"\"\n",
    "        self.choices = []\n",
    "        x_shape = self.dataset[0][0].shape\n",
    "        N = len(self)\n",
    "\n",
    "        for t in self.transforms:\n",
    "            self.choices.append(t.set_random_choices(N, x_shape))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBatches:\n",
    "    '''\n",
    "    Creates a dataloader using the specificed data frame with the dataset corresponding to \"data\".\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df, transforms, shuffle, img_folder_path, idx=IDX, batch_size=16, num_workers=8,\n",
    "                 drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='supervised', N=None):\n",
    "\n",
    "        if problem_type=='supervised':\n",
    "            self.dataset = Transform(LabeledDataSet(df, image_path=img_folder_path, idx=idx),\n",
    "                                     transforms=transforms, normalize=normalize, seed=seed, r_pix=r_pix)\n",
    "        elif problem_type=='unsupervised':\n",
    "            self.dataset = UnlabeledTransform(UnlabeledDataSet(df, image_path=img_folder_path, N=N),\n",
    "                                     transforms=transforms, normalize=normalize, seed=seed, r_pix=r_pix)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True,\n",
    "            shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "\n",
    "    def __iter__(self): return ((x.cuda().float(), y.cuda().float()) for (x, y) in self.dataloader)\n",
    "\n",
    "    def __len__(self): return len(self.dataloader)\n",
    "\n",
    "    def set_random_choices(self):\n",
    "        if hasattr(self.dataset, \"set_random_choices\"): self.dataset.set_random_choices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_training = DataBatches(chesxray_train_df_balanced, TRANSFORMATIONS, idx=IDX, shuffle=True, img_folder_path=CHESTXRAY_FOLDER, batch_size=16, num_workers=8,\n",
    "#                  drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='supervised')\n",
    "\n",
    "# labeled_training.set_random_choices()\n",
    "# x,y = next(iter(labeled_training))\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabeled_training = DataBatches(chexpert_train_df, TRANSFORMATIONS, shuffle=True, img_folder_path=CHEXPERT_FOLDER, batch_size=16, num_workers=8,\n",
    "#                  drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='unsupervised')\n",
    "\n",
    "# unlabeled_training.set_random_choices()\n",
    "# x,y = next(iter(unlabeled_training))\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_with_logits(logit, logit_t):\n",
    "    p = F.softmax(logit, dim=1)\n",
    "    log_p = F.log_softmax(logit, dim=1)\n",
    "    log_q = F.log_softmax(logit_t, dim=1)\n",
    "    kl = (p * (log_p - log_q)).sum(1).mean()\n",
    "    return kl\n",
    "\n",
    "def train(n_epochs, train_dl, unsuper_dl, valid_dl, model, max_lr=.01, wd=0, alpha=1./ 3,\n",
    "          save_path=None, unfreeze_during_loop:tuple=None):\n",
    "    \n",
    "    if unfreeze_during_loop:\n",
    "        total_iter = n_epochs*len(train_dl)\n",
    "        first_unfreeze = int(total_iter*unfreeze_during_loop[0])\n",
    "        second_unfreeze = int(total_iter*unfreeze_during_loop[1])\n",
    "\n",
    "    best_loss = np.inf\n",
    "    cnt = 0\n",
    "    \n",
    "    policy = TrainingPolicy(n_epochs=n_epochs, dl=train_dl, max_lr=max_lr)\n",
    "    optimizer = OptimizerWrapper(model, policy, wd=wd, alpha=alpha)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(n_epochs), ):\n",
    "        model.train()\n",
    "        agg_div = 0\n",
    "        agg_loss = 0\n",
    "        train_dl.set_random_choices()\n",
    "        unsuper_dl.set_random_choices()\n",
    "        for (x,y), (x1,x2) in tqdm_notebook(zip(train_dl, unsuper_dl), leave=False):\n",
    "\n",
    "            if unfreeze_during_loop:\n",
    "                if cnt == first_unfreeze: model.unfreeze(1)\n",
    "                if cnt == second_unfreeze: model.unfreeze(0)\n",
    "\n",
    "#           first loss\n",
    "            out = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(input=out.squeeze(), target=y)\n",
    "            \n",
    "            \n",
    "#           second loss\n",
    "            x = x1\n",
    "            with torch.no_grad(): logit1 = model(x)\n",
    "            logit2 = model(x2)\n",
    "            loss += kl_divergence_with_logits(logit1, logit2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch = y.shape[0]\n",
    "            agg_loss += batch*loss.item()\n",
    "            agg_div += batch\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        val_loss, measure, _ = validate_binary(model, valid_dl)\n",
    "        print(f'Ep. {epoch+1} - train loss {agg_loss/agg_div:.4f} -  val loss {val_loss:.4f} AUC {measure:.4f}')\n",
    "\n",
    "        if save_path and val_loss < best_loss:\n",
    "            save_model(model, save_path)\n",
    "            best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chesxray_valid_df_balanced = subset_df(chesxray_valid_df, amt=None, idx=IDX)\n",
    "chesxray_test_df_balanced = subset_df(chesxray_test_df, amt=None, idx=IDX)\n",
    "\n",
    "N = 50\n",
    "chesxray_train_df_balanced = subset_df(chesxray_train_df, amt=N, idx=IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_training = DataBatches(chesxray_train_df_balanced, TRANSFORMATIONS, idx=IDX, shuffle=True, img_folder_path=CHESTXRAY_FOLDER, batch_size=16, num_workers=8,\n",
    "                 drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='supervised')\n",
    "\n",
    "labeled_validation = DataBatches(chesxray_valid_df_balanced, None, idx=IDX, shuffle=False, img_folder_path=CHESTXRAY_FOLDER, batch_size=16, num_workers=8,\n",
    "                 drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='supervised')\n",
    "\n",
    "unlabeled_training = DataBatches(chexpert_train_df, TRANSFORMATIONS, shuffle=True, img_folder_path=CHEXPERT_FOLDER, batch_size=16, num_workers=8,\n",
    "                 drop_last=False, r_pix=8, normalize=True, seed=42, problem_type='unsupervised', N = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7f61a0afed45bf969cab6691a73a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 - train loss 0.7352 -  val loss 0.6716 AUC 0.6383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 2 - train loss 0.6793 -  val loss 2.3522 AUC 0.5693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 3 - train loss 1.0804 -  val loss 21.7729 AUC 0.5334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 4 - train loss 0.7966 -  val loss 438.3789 AUC 0.5119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 5 - train loss 0.6626 -  val loss 3.1003 AUC 0.5437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 6 - train loss 0.6038 -  val loss 7.9705 AUC 0.5223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 7 - train loss 0.5716 -  val loss 0.9217 AUC 0.5812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 8 - train loss 0.5750 -  val loss 1.1793 AUC 0.6011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 9 - train loss 0.5751 -  val loss 0.7011 AUC 0.6230\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 10 - train loss 0.5358 -  val loss 0.7029 AUC 0.6140\n"
     ]
    }
   ],
   "source": [
    "pretrained = True\n",
    "freeze = True\n",
    "\n",
    "dn121 = DenseNet121(1, pretrained=pretrained, freeze=freeze).cuda()\n",
    "\n",
    "train(10, train_dl=labeled_training, unsuper_dl=unlabeled_training, valid_dl=labeled_validation, model=dn121, max_lr=.01, wd=0, alpha=1./ 3,\n",
    "          save_path=None, unfreeze_during_loop=(.1,.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
