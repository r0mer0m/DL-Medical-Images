{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we illustrate how **regular training** is performed in our experiments, write the script to train in a range of (small) training samples and plot the results of this script.\n",
    "\n",
    "## How is it organized?\n",
    "\n",
    "1. In the first section notebook we illustrate the scenario were ImageNet is used with gradual unfreezing for the whole dataset. This is useful to test how the model behaves when all the data is available. \n",
    "\n",
    "2. In the second section the script is written and we explore different alternatives in transfer learning for a range of training data. We do so by combining regular training with multiple alternatives in transfer learning:\n",
    "\n",
    "    * Is transfer learning used? If so, from what dataset?  We contemplate ImageNet and MURA (general and medical images respectively).\n",
    "    * How is transfer learning done? previous CNN as feature extractor, fine-tune CNN? If we fine-tune, do we use differential learning rates?\n",
    "    * Do we do progressive unfreezing?\n",
    "\n",
    " Some of those options are excluding and other are complementary. For instance, no transfer learning and using the previous CNN as feature extractor are excluding.\n",
    "\n",
    "3. In the last section we plot the results of running the script.\n",
    "\n",
    "## I want to dig deeper\n",
    "\n",
    "This notebook's main purpose is illustrating how **regular training** is used. All other aspects are imported from `data_manipulation`,`utils`, `architectures` and `train_functions`. If the reader wants to dig dipper in aspects such as how data augmentation is implemented, what policy we use for the learning rate or how the optimizer is used we point them to the modules mentioned above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all the available data\n",
    "## Imports & global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import sys; sys.path.append(\"/data/miguel/practicum/DL-Medical-Physics\")\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from core import *\n",
    "from data_manipulation import DataBatches, RandomRotation, Flip, RandomCrop, balance_obs, multi_label_2_binary\n",
    "from utils import save_model, load_model, lr_loss_plot\n",
    "from architectures import DenseNet121\n",
    "from train_functions import get_optimizer, FinderPolicy, OptimizerWrapper, validate_binary, lr_finder, TTA_binary\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "TRANSFORMATIONS = [RandomRotation(arc_width=20), Flip(), RandomCrop(r_pix=8)]\n",
    "PRETRAINED = True\n",
    "\n",
    "BASE_PATH = Path('/data/miguel/practicum/')\n",
    "PATH = BASE_PATH/'data'\n",
    "# SAVE_DATA = BASE_PATH/'output/real_data_experiments/multilabel/results'\n",
    "# SAVE_DIRECTORY = BASE_PATH/'output/real_data_experiments/multilabel/models'\n",
    "# SAVE_PLOT = Path('../latest_plots/14diseases-app1')\n",
    "\n",
    "IMG_FOLDER = PATH/'ChestXRay-250'\n",
    "DATA = 'Pneumonia'\n",
    "DISEASE = 'Emphysema'\n",
    "\n",
    "idx2tgt = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "               'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "\n",
    "tgt2idx = {disease: i for i, disease in enumerate(idx2tgt)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "train_df = pd.read_csv(PATH/\"train_df.csv\")\n",
    "valid_df = pd.read_csv(PATH/\"val_df.csv\")\n",
    "test_df = pd.read_csv(PATH/\"test_df.csv\")\n",
    "\n",
    "train_df = multi_label_2_binary(train_df, tgt2idx[DISEASE])\n",
    "sample_train_df = balance_obs(train_df, amt=2000)\n",
    "\n",
    "valid_df = multi_label_2_binary(valid_df, tgt2idx[DISEASE])\n",
    "valid_df = balance_obs(valid_df, amt=2*len(valid_df[valid_df['Label']==1]))\n",
    "\n",
    "test_df = multi_label_2_binary(test_df, tgt2idx[DISEASE])\n",
    "test_df = balance_obs(test_df, amt=2*len(test_df[test_df['Label']==1]))\n",
    "\n",
    "train_dl = DataBatches(train_df, img_folder_path=IMG_FOLDER,transforms=TRANSFORMATIONS, \n",
    "                       shuffle=True, data=DATA,batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "valid_dl = DataBatches(valid_df,img_folder_path=IMG_FOLDER, transforms = False,\n",
    "                       shuffle = False, data=DATA, batch_size = BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "test_dl = DataBatches(test_df,img_folder_path=IMG_FOLDER, transforms = TRANSFORMATIONS, \n",
    "                      shuffle = False, data=DATA, batch_size = BATCH_SIZE, normalize=PRETRAINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cefe70a9254549a9c45f1f6d6f02a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'set_random_choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d179c1bef172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRETRAINED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlr_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/practicum/DL-Medical-Physics/train_functions.py\u001b[0m in \u001b[0;36mlr_finder\u001b[0;34m(model, n_epochs, train_dl, min_lr, max_lr, save_path, early_stopping, plot_every)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_choices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'set_random_choices'"
     ]
    }
   ],
   "source": [
    "model = DenseNet121(1, pretrained=PRETRAINED, freeze=False).cuda()\n",
    "lrs, losses  = lr_finder(model, 1, sample_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_annealing(start_lr, end_lr, n_iterations):\n",
    "    i = np.arange(n_iterations)\n",
    "    c_i = 1 + np.cos(i * np.pi / n_iterations)\n",
    "    return end_lr + (start_lr - end_lr) / 2 * c_i\n",
    "\n",
    "class TrainingPolicy:\n",
    "    '''Cretes the lr and momentum policy'''\n",
    "\n",
    "    def __init__(self, n_epochs, dl, max_lr, pctg=.3, moms=(.95, .85),\n",
    "                 delta=1e-4, div_factor=25.):\n",
    "        \n",
    "        total_iterations = n_epochs * len(dl)\n",
    "\n",
    "        iter1 = int(total_iterations * pctg)\n",
    "        iter2 = total_iterations - int(total_iterations * pctg)\n",
    "        iterations = (iter1, iter2)\n",
    "\n",
    "        min_start = max_lr / div_factor\n",
    "        min_end = min_start * delta\n",
    "\n",
    "        lr_segments = ((min_start, max_lr), (max_lr, min_end))\n",
    "        mom_segments = (moms, (moms[1], moms[0]))\n",
    "\n",
    "        self.lr_schedule = self._create_schedule(lr_segments, iterations)\n",
    "        self.mom_schedule = self._create_schedule(mom_segments, iterations)\n",
    "\n",
    "        self.idx = -1\n",
    "\n",
    "    def _create_schedule(self, segments, iterations):\n",
    "        '''\n",
    "        Creates a schedule given a function, behaviour and size\n",
    "        '''\n",
    "        stages = [cos_annealing(start, end, n) for ((start, end), n) in zip(segments, iterations)]\n",
    "        return np.concatenate(stages)\n",
    "\n",
    "    def step(self):\n",
    "        self.idx += 1\n",
    "        return self.lr_schedule[self.idx], self.mom_schedule[self.idx]\n",
    "    \n",
    "def one_cycle_train(n_epochs, train_dl, valid_dl, model, max_lr=.01, wd=0, alpha=1./ 3,\n",
    "          save_path=None, unfreeze_during_loop:tuple=None):\n",
    "    \n",
    "    if unfreeze_during_loop:\n",
    "        total_iter = n_epochs*len(train_dl)\n",
    "        first_unfreeze = int(total_iter*unfreeze_during_loop[0])\n",
    "        second_unfreeze = int(total_iter*unfreeze_during_loop[1])\n",
    "\n",
    "    best_loss = np.inf\n",
    "    cnt = 0\n",
    "    \n",
    "    policy = TrainingPolicy(n_epochs=n_epochs, dl=train_dl, max_lr=max_lr)\n",
    "    optimizer = OptimizerWrapper(model, policy, wd=wd, alpha=alpha)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(n_epochs), ):\n",
    "        model.train()\n",
    "        agg_div = 0\n",
    "        agg_loss = 0\n",
    "        train_dl.set_random_choices()\n",
    "        for x, y in tqdm_notebook(train_dl, leave=False):\n",
    "\n",
    "            if unfreeze_during_loop:\n",
    "                if cnt == first_unfreeze: model.unfreeze(1)\n",
    "                if cnt == second_unfreeze: model.unfreeze(0)\n",
    "\n",
    "            out = model(x).squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(input=out, target=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch = y.shape[0]\n",
    "            agg_loss += batch*loss.item()\n",
    "            agg_div += batch\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        val_loss, measure, _ = validate_binary(model, valid_dl)\n",
    "        print(f'Ep. {epoch+1} - train loss {agg_loss/agg_div:.4f} -  val loss {val_loss:.4f} AUC {measure:.4f}')\n",
    "\n",
    "        if save_path and val_loss < best_loss:\n",
    "            save_model(model, save_path)\n",
    "            best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No transfer learning (done in previous experiment)\n",
    "* CNN as feature extractor.\n",
    "* Fine-tune all CNN at once, equal learning rates.\n",
    "* Gradual unfreezing with differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN as feature extractor\n",
    "freeze = True\n",
    "gradual_unfreezing = False\n",
    "\n",
    "train_dl = DataBatches(sample_train_df, img_folder_path=IMG_FOLDER,transforms=TRANSFORMATIONS,\n",
    "                       shuffle=True, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_weight_distribution(model):\n",
    "    for k,v in model.state_dict().items():\n",
    "        if 'weight' in k or 'bias' in k: # All layers\n",
    "    #     if ('running_mean' not in k) and ('running_var' not in k) and ('num_batches_tracked' not in k): # All but batchnorm\n",
    "            W = model.state_dict()[k].data\n",
    "            w_shape = W.shape\n",
    "#             print(k, W.shape)\n",
    "\n",
    "            mu = W.mean()\n",
    "            sigma = W.mean()\n",
    "        \n",
    "            for _ in w_shape[1:]: mu = mu.unsqueeze(-1)\n",
    "            for _ in w_shape[1:]: sigma = sigma.unsqueeze(-1)\n",
    "            \n",
    "#             print(W.shape, sigma.shape,mu.shape)\n",
    "            W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "            model.state_dict()[k].data.copy_(W_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "\n",
    "keep_only_weight_distribution(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in model.state_dict().items():print(k,v.shape)\n",
    "# list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "one_cycle_train(EPOCHS, train_dl, valid_dl, model, max_lr=.001, save_path=None, unfreeze_during_loop=(.1, .2) if gradual_unfreezing else None, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune all CNN at once, equal learning rates\n",
    "freeze = False\n",
    "gradual_unfreezing = False\n",
    "\n",
    "train_dl = DataBatches(sample_train_df, img_folder_path=IMG_FOLDER,transforms=TRANSFORMATIONS,\n",
    "                       shuffle=True, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "save_path = None\n",
    "one_cycle_train(EPOCHS, train_dl, valid_dl, model, max_lr=.001, save_path=None, unfreeze_during_loop=(.1, .2) if gradual_unfreezing else None, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradual unfreezing with discriminative learning\n",
    "freeze = True\n",
    "gradual_unfreezing = True\n",
    "\n",
    "train_dl = DataBatches(sample_train_df, img_folder_path=IMG_FOLDER,transforms=TRANSFORMATIONS,\n",
    "                       shuffle=True, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "save_path = None\n",
    "one_cycle_train(EPOCHS, train_dl, valid_dl, model, max_lr=.001, save_path=None, unfreeze_during_loop=(.1, .2) if gradual_unfreezing else None, alpha=1./3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_model(model, save_pathc)\n",
    "# test_dl = DataBatches(test_df,img_folder_path=IMG_FOLDER, transforms = TRANSFORMATIONS, \n",
    "#                       shuffle = False, data=DATA, batch_size = BATCH_SIZE, normalize=PRETRAINED)\n",
    "# TTA_multilabel(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample range: Writing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the different combinations on a script. Observe that we have constructed the training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting emphysema.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile emphysema.py\n",
    "\n",
    "# APPROACH 1 : All parameters - Does not work - outputs NAN\n",
    "# APPROACH 2 : Only for parameters tag as \"wegight\" - \"bias\"\n",
    "# APPROACH 3 : 2 + no norm tag (i.e. not batch norm)\n",
    "# APPROACH 4 : 2 + channel wise\n",
    "# APPROACH 3 : 2 + channel wise + no norm tag (i.e. not batch norm)\n",
    "\n",
    "\n",
    "import sys; sys.path.append(\"/data/miguel/practicum/DL-Medical-Physics\")\n",
    "import argparse\n",
    "\n",
    "from core import *\n",
    "from data_manipulation import DataBatches, RandomRotation, Flip, RandomCrop, balance_obs, multi_label_2_binary\n",
    "from utils import save_model, load_model, lr_loss_plot\n",
    "from architectures import DenseNet121\n",
    "from train_functions import get_optimizer, FinderPolicy, OptimizerWrapper, validate_binary, lr_finder, TTA_binary\n",
    "import json\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "TRANSFORMATIONS = [RandomRotation(arc_width=20), Flip(), RandomCrop(r_pix=8)]\n",
    "DATA = 'Pneumonia'\n",
    "DISEASE = 'Emphysema'\n",
    "\n",
    "idx2tgt = ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "               'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "\n",
    "tgt2idx = {disease: i for i, disease in enumerate(idx2tgt)}\n",
    "\n",
    "SAMPLE_AMOUNTS = [50,100,200,400,600,800,1000,1200,1400,1600,1800,2000]\n",
    "\n",
    "BASE_PATH = Path('/data/miguel/practicum/')\n",
    "PATH = BASE_PATH/'data'\n",
    "SAVE_DIRECTORY = BASE_PATH/'DL-Medical-Physics/distribution_transfer_learning/models'\n",
    "SAVE_DATA = BASE_PATH/'DL-Medical-Physics/distribution_transfer_learning/results'\n",
    "IMG_FOLDER = PATH/'ChestXRay-250'\n",
    "PRETRAINED = True # Imagenet\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--approach', help='Approach when randomizing')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args)\n",
    "\n",
    "APPROACH = int(args.approach)\n",
    "\n",
    "def cos_annealing(start_lr, end_lr, n_iterations):\n",
    "    i = np.arange(n_iterations)\n",
    "    c_i = 1 + np.cos(i * np.pi / n_iterations)\n",
    "    return end_lr + (start_lr - end_lr) / 2 * c_i\n",
    "\n",
    "class TrainingPolicy:\n",
    "    '''Cretes the lr and momentum policy'''\n",
    "\n",
    "    def __init__(self, n_epochs, dl, max_lr, pctg=.3, moms=(.95, .85),\n",
    "                 delta=1e-4, div_factor=25.):\n",
    "        \n",
    "        total_iterations = n_epochs * len(dl)\n",
    "\n",
    "        iter1 = int(total_iterations * pctg)\n",
    "        iter2 = total_iterations - int(total_iterations * pctg)\n",
    "        iterations = (iter1, iter2)\n",
    "\n",
    "        min_start = max_lr / div_factor\n",
    "        min_end = min_start * delta\n",
    "\n",
    "        lr_segments = ((min_start, max_lr), (max_lr, min_end))\n",
    "        mom_segments = (moms, (moms[1], moms[0]))\n",
    "\n",
    "        self.lr_schedule = self._create_schedule(lr_segments, iterations)\n",
    "        self.mom_schedule = self._create_schedule(mom_segments, iterations)\n",
    "\n",
    "        self.idx = -1\n",
    "\n",
    "    def _create_schedule(self, segments, iterations):\n",
    "        '''\n",
    "        Creates a schedule given a function, behaviour and size\n",
    "        '''\n",
    "        stages = [cos_annealing(start, end, n) for ((start, end), n) in zip(segments, iterations)]\n",
    "        return np.concatenate(stages)\n",
    "\n",
    "    def step(self):\n",
    "        self.idx += 1\n",
    "        return self.lr_schedule[self.idx], self.mom_schedule[self.idx]\n",
    "    \n",
    "def one_cycle_train(n_epochs, train_dl, valid_dl, model, max_lr=.01, wd=0, alpha=1./ 3,\n",
    "          save_path=None, unfreeze_during_loop:tuple=None):\n",
    "    \n",
    "    if unfreeze_during_loop:\n",
    "        total_iter = n_epochs*len(train_dl)\n",
    "        first_unfreeze = int(total_iter*unfreeze_during_loop[0])\n",
    "        second_unfreeze = int(total_iter*unfreeze_during_loop[1])\n",
    "\n",
    "    best_loss = np.inf\n",
    "    cnt = 0\n",
    "    \n",
    "    policy = TrainingPolicy(n_epochs=n_epochs, dl=train_dl, max_lr=max_lr)\n",
    "    optimizer = OptimizerWrapper(model, policy, wd=wd, alpha=alpha)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        agg_div = 0\n",
    "        agg_loss = 0\n",
    "        train_dl.set_random_choices()\n",
    "        for x, y in train_dl:\n",
    "\n",
    "            if unfreeze_during_loop:\n",
    "                if cnt == first_unfreeze: model.unfreeze(1)\n",
    "                if cnt == second_unfreeze: model.unfreeze(0)\n",
    "\n",
    "            out = model(x).squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(input=out, target=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch = y.shape[0]\n",
    "            agg_loss += batch*loss.item()\n",
    "            agg_div += batch\n",
    "            cnt += 1\n",
    "        \n",
    "        val_loss, measure, _ = validate_binary(model, valid_dl)\n",
    "        print(f'Ep. {epoch+1} - train loss {agg_loss/agg_div:.4f} -  val loss {val_loss:.4f} AUC {measure:.4f}')\n",
    "        \n",
    "        if save_path and val_loss < best_loss:\n",
    "            save_model(model, save_path)\n",
    "            best_loss = val_loss\n",
    "\n",
    "class keep_only_weight_distribution:\n",
    "    \n",
    "    def __init__(self, approach=2):\n",
    "        print(approach, approach==5)\n",
    "        if approach==2: self.randomize = self.randomize_app2\n",
    "        elif approach==3: self.randomize = self.randomize_app3\n",
    "        elif approach==4: self.randomize = self.randomize_app4\n",
    "        elif approach==5: self.randomize = self.randomize_app5\n",
    "        elif approach==6: self.randomize = self.randomize_app6\n",
    "    \n",
    "    \n",
    "    def randomize_app2(self, model):\n",
    "        for k,v in model.state_dict().items():\n",
    "            if 'weight' in k or 'bias' in k: # All layers - with weights or bias\n",
    "                W = model.state_dict()[k].data\n",
    "                w_shape = W.shape\n",
    "\n",
    "                mu = W.mean()\n",
    "                sigma = W.mean()\n",
    "\n",
    "                W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "                model.state_dict()[k].data.copy_(W_new)\n",
    "    \n",
    "    def randomize_app3(self, model):\n",
    "        for k,v in model.state_dict().items():\n",
    "            if 'weight' in k or 'bias' in k: # All layers\n",
    "                W = model.state_dict()[k].data\n",
    "                w_shape = W.shape\n",
    "\n",
    "                mu = W.view(W.shape[0],-1).mean(-1)\n",
    "                sigma = W.view(W.shape[0],-1).mean()\n",
    "                \n",
    "                for _ in W.shape[1:]: \n",
    "                    mu = mu.unsqueeze(-1)\n",
    "                    sigma = sigma.unsqueeze(-1)\n",
    "\n",
    "                W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "                model.state_dict()[k].data.copy_(W_new)\n",
    "                \n",
    "    def randomize_app4(self, model):\n",
    "            for k,v in model.state_dict().items():\n",
    "                if ('norm' not in k) and ('weight' in k or 'bias' in k): # All layers - with weights or bias no norm\n",
    "                    W = model.state_dict()[k].data\n",
    "                    w_shape = W.shape\n",
    "\n",
    "                    mu = W.mean()\n",
    "                    sigma = W.mean()\n",
    "\n",
    "                    W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "                    model.state_dict()[k].data.copy_(W_new)\n",
    "                    \n",
    "    def randomize_app5(self, model):\n",
    "            for k,v in model.state_dict().items():\n",
    "                if ('norm' not in k) and ('weight' in k or 'bias' in k): # All layers - with weights or bias no norm\n",
    "                    W = model.state_dict()[k].data\n",
    "                    w_shape = W.shape\n",
    "\n",
    "                    mu = W.view(W.shape[0],-1).mean(-1)\n",
    "                    sigma = W.view(W.shape[0],-1).mean()\n",
    "\n",
    "                    for _ in W.shape[1:]: \n",
    "                        mu = mu.unsqueeze(-1)\n",
    "                        sigma = sigma.unsqueeze(-1)\n",
    "\n",
    "                    W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "                    model.state_dict()[k].data.copy_(W_new)\n",
    "    \n",
    "    def randomize_app6(self, model):\n",
    "            for k,v in model.state_dict().items():\n",
    "                if ('norm' not in k) and ('weight' in k or 'bias' in k): # All layers - with weights or bias no norm\n",
    "                    W = model.state_dict()[k].data\n",
    "                    w_shape = W.shape\n",
    "\n",
    "                    mu = W.view(W.shape[0],-1).mean(-1)\n",
    "                    sigma = W.view(W.shape[0],-1).mean()\n",
    "\n",
    "                    for _ in W.shape[1:]: \n",
    "                        mu = mu.unsqueeze(-1)\n",
    "                        sigma = sigma.unsqueeze(-1)\n",
    "\n",
    "                    W_new = sigma*torch.randn_like(W)+mu\n",
    "\n",
    "                    model.state_dict()[k].data.copy_(W_new)\n",
    "                if ('norm' in k):\n",
    "                    if 'weight' in k: # All layers - with weights or bias no norm\n",
    "                        W = model.state_dict()[k].data\n",
    "                        \n",
    "#                         W_new = torch.Tensor(W.shape)\n",
    "#                         W_new = torch.ones_like(W)\n",
    "                        W_new = torch.rand(W.shape)\n",
    "                        \n",
    "                        model.state_dict()[k].data.copy_(W_new)\n",
    "                    \n",
    "                    if 'bias' in k:\n",
    "                        W = model.state_dict()[k].data\n",
    "                        \n",
    "#                         W_new = torch.Tensor(W.shape)\n",
    "                        W_new = torch.zeros_like(W)\n",
    "                        \n",
    "                        model.state_dict()[k].data.copy_(W_new)\n",
    "\n",
    "# # Training            \n",
    "train_df = pd.read_csv(PATH/\"train_df.csv\")\n",
    "valid_df = pd.read_csv(PATH/\"val_df.csv\")\n",
    "\n",
    "train_df = multi_label_2_binary(train_df, tgt2idx[DISEASE])\n",
    "\n",
    "valid_df = multi_label_2_binary(valid_df, tgt2idx[DISEASE])\n",
    "valid_df = balance_obs(valid_df, amt=2*len(valid_df[valid_df['Label']==1]))\n",
    "\n",
    "valid_dl = DataBatches(valid_df,img_folder_path=IMG_FOLDER,transforms=False, \n",
    "                       shuffle=False, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "train_df = train_df.sample(frac=1)\n",
    "\n",
    "freeze = True\n",
    "gradual_unfreezing = True\n",
    "\n",
    "keep_dist = keep_only_weight_distribution(approach=APPROACH)\n",
    "\n",
    "for N in SAMPLE_AMOUNTS:\n",
    "\n",
    "    df = balance_obs(train_df, amt=N)\n",
    "\n",
    "    train_dl = DataBatches(df, img_folder_path=IMG_FOLDER, transforms=TRANSFORMATIONS, \n",
    "                           shuffle=True, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "    \n",
    "    # normal ImageNet\n",
    "    model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "\n",
    "    save_path = SAVE_DIRECTORY/f\"{DISEASE.lower()}-std-imgnet-{N}-{APPROACH}.pth\"\n",
    "    \n",
    "    one_cycle_train(EPOCHS, train_dl, valid_dl, model, max_lr=.001, save_path=save_path, unfreeze_during_loop=(.1, .2) if gradual_unfreezing else None, alpha=1)\n",
    "    \n",
    "    # distribution ImageNet\n",
    "    model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "    \n",
    "    keep_dist.randomize(model)\n",
    "\n",
    "    save_path = SAVE_DIRECTORY/f\"{DISEASE.lower()}-dist-imgnet-{N}-{APPROACH}.pth\"\n",
    "    \n",
    "    one_cycle_train(EPOCHS, train_dl, valid_dl, model, max_lr=.001, save_path=save_path, unfreeze_during_loop=(.1, .2) if gradual_unfreezing else None, alpha=1./3)\n",
    "    \n",
    "# Evaluation\n",
    "\n",
    "std_imgnet = {\n",
    "    'losses':[],\n",
    "    'aucs':[]\n",
    "}\n",
    "dist_imgnet = {\n",
    "    'losses':[],\n",
    "    'aucs':[]\n",
    "}\n",
    "\n",
    "test_df = pd.read_csv(PATH/\"test_df.csv\")\n",
    "test_df = multi_label_2_binary(test_df, tgt2idx[DISEASE])\n",
    "test_df = balance_obs(test_df, amt=2*len(test_df[test_df['Label']==1]))\n",
    "\n",
    "test_dl = DataBatches(test_df,img_folder_path=IMG_FOLDER, transforms=TRANSFORMATIONS, \n",
    "                      shuffle=False, data=DATA, batch_size=BATCH_SIZE, normalize=PRETRAINED)\n",
    "\n",
    "model = DenseNet121(1, pretrained=PRETRAINED, freeze=freeze).cuda()\n",
    "\n",
    "for i, N in enumerate(SAMPLE_AMOUNTS):\n",
    "\n",
    "    # std imgnet\n",
    "    load_path = SAVE_DIRECTORY/f\"{DISEASE.lower()}-std-imgnet-{N}-{APPROACH}.pth\"\n",
    "\n",
    "    load_model(model, load_path)\n",
    "\n",
    "    loss, mean_auc, _ = TTA_binary(model, test_dl, ndl=4)\n",
    "\n",
    "    std_imgnet['losses'].append(loss)\n",
    "    std_imgnet['aucs'].append(mean_auc)\n",
    "    \n",
    "    # dist imgnet\n",
    "    load_path = SAVE_DIRECTORY/f\"{DISEASE.lower()}-dist-imgnet-{N}-{APPROACH}.pth\"\n",
    "\n",
    "    load_model(model, load_path)\n",
    "\n",
    "    loss, mean_auc, _ = TTA_binary(model, test_dl, ndl=4)\n",
    "\n",
    "    dist_imgnet['losses'].append(loss)\n",
    "    dist_imgnet['aucs'].append(mean_auc)\n",
    "\n",
    "\n",
    "std_imgnet = json.dumps(std_imgnet)\n",
    "with open(f'results/{DISEASE.lower()}_std-imgnet-{APPROACH}.json', 'w') as f:\n",
    "    f.write(std_imgnet)\n",
    "    \n",
    "dist_imgnet = json.dumps(dist_imgnet)\n",
    "with open(f'results/{DISEASE.lower()}_dist-imgnet-{APPROACH}.json', 'w') as f:\n",
    "    f.write(dist_imgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
